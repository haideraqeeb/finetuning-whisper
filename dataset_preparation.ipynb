{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOeNCgWAohx7"
      },
      "source": [
        "# Dataset Preparation\n",
        "The Gujarati dataset, approximately 17 gigabytes in size, was collected from the provided link and subsequently processed. The dataset was organized into an **audio folder** containing the audio files. Alongside this folder, three additional directories—**train**, **eval**, and **dev**—were present. Each of these directories included files formatted in the **Kaldi** standard.\n",
        "\n",
        "- **segments**: maps audio segments to start and end times accurately\n",
        "- **text**: stores transcriptions of utterances corresponding to audio segments\n",
        "- **wav.scp**: provides file paths or commands to access audio files\n",
        "- **spk2utt**: lists utterance IDs for each speaker in the dataset\n",
        "- **utt2spk**: links each utterance ID to its corresponding speaker ID\n",
        "- **utt2dur**: specifies duration of each utterance in seconds for reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL27Auqxj_o-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5h820G8kN-x"
      },
      "source": [
        "Make sure that the .tar.gz zip file is present and extracted in drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3EFoXqwlN9E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import notebook_login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpd-8mn7owSb"
      },
      "source": [
        "First the audio is converted into consistent format of 16kHz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiCY5d-1kX5s"
      },
      "outputs": [],
      "source": [
        "preprocessed_audio_dir = '/content/drive/MyDrive/preprocessed_audio'\n",
        "os.makedirs(preprocessed_audio_dir, exist_ok=True)\n",
        "\n",
        "def preprocess_audio(input_dir, output_dir):\n",
        "    audio_dir = Path('/content/drive/MyDrive/extracted_dataset/SPRING_INX_Gujarati_R1/Audio')\n",
        "\n",
        "    audio_files = list(audio_dir.rglob(\"*.wav\"))\n",
        "\n",
        "    for file in audio_files:\n",
        "        print(f\"Processing file: {file}\")\n",
        "\n",
        "        y, sr = librosa.load(file, sr=16000)\n",
        "\n",
        "        output_path = os.path.join(output_dir, file.name)\n",
        "\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        sf.write(output_path, y, 16000)\n",
        "        print(f\"Saved to {output_path}\")\n",
        "\n",
        "preprocess_audio('/content/drive/MyDrive/extracted_dataset/SPRING_INX_Gujarati_R1/train', preprocessed_audio_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmXuZ5GhkyX4"
      },
      "source": [
        "After executing this cell, the data must be in the following folder ```'/content/drive/MyDrive/preprocessed_audio'```\n",
        "\n",
        "Following this we will create the metadata.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyuJg70Kk-8-"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/preprocessed_audio\"\n",
        "subsets = [\"train\", \"dev\", \"eval\"]\n",
        "\n",
        "metadata_entries = []\n",
        "\n",
        "for subset in subsets:\n",
        "    subset_path = os.path.join(base_path, subset)\n",
        "\n",
        "    wav_scp_path = os.path.join(subset_path, \"wav.scp\")\n",
        "    segments_path = os.path.join(subset_path, \"segments\")\n",
        "    text_path = os.path.join(subset_path, \"text\")\n",
        "    utt2dur_path = os.path.join(subset_path, \"utt2dur\")\n",
        "\n",
        "    for file_path in [wav_scp_path, segments_path, text_path, utt2dur_path]:\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"Required file not found: {file_path}\")\n",
        "\n",
        "    with open(wav_scp_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        wav_scp = {line.split()[0]: line.split()[1] for line in f}\n",
        "\n",
        "    with open(segments_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        segments = {\n",
        "            line.split()[0]: {\n",
        "                \"file_name\": line.split()[1],\n",
        "                \"start_time\": float(line.split()[2]),\n",
        "                \"end_time\": float(line.split()[3]),\n",
        "            }\n",
        "            for line in f\n",
        "        }\n",
        "\n",
        "    with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        transcriptions = {line.split(maxsplit=1)[0]: line.split(maxsplit=1)[1].strip() for line in f}\n",
        "\n",
        "    with open(utt2dur_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        durations = {line.split()[0]: float(line.split()[1]) for line in f}\n",
        "\n",
        "    for utt_id, segment_info in segments.items():\n",
        "        file_name = segment_info[\"file_name\"]\n",
        "        start_time = segment_info[\"start_time\"]\n",
        "        end_time = segment_info[\"end_time\"]\n",
        "\n",
        "        metadata_entries.append(\n",
        "            {\n",
        "                \"audio_file_path\": wav_scp[file_name],\n",
        "                \"transcription\": transcriptions.get(utt_id, \"\"),\n",
        "                \"duration\": durations.get(utt_id, end_time - start_time),\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "            }\n",
        "        )\n",
        "\n",
        "metadata_df = pd.DataFrame(metadata_entries)\n",
        "\n",
        "output_csv_path = os.path.join(base_path, \"metadata.csv\")\n",
        "metadata_df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Metadata CSV saved to: {output_csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6SZ1kjlpVX"
      },
      "source": [
        "Finally the next cell will process the audio files so that they are saved into ```\"/content/drive/MyDrive/processed_audio\"```\n",
        "\n",
        "This will take each and every audio file and then using the segmenst and the utt2dur files cut the audio into each utterance and make the new audio file as the utterance name. This is done because the whisper model does not take in audios of length more than 30 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVLp5CNalo8E"
      },
      "outputs": [],
      "source": [
        "def split_audio_and_generate_metadata(subset, base_dir, output_dir, processed_audio_dir):\n",
        "    \"\"\"\n",
        "    this func splits audio files into utterances based on segments\n",
        "\n",
        "    args:\n",
        "    subset (str): subset folder ('train', 'eval', 'dev').\n",
        "    base_dir (str): base directory of the dataset.\n",
        "    output_dir (str): directory to save the metadata CSV files.\n",
        "    processed_audio_dir (str): directory to save processed audio files.\n",
        "\n",
        "    \"\"\"\n",
        "    subset_dir = os.path.join(base_dir, subset)\n",
        "    wav_scp_path = os.path.join(subset_dir, \"wav.scp\")\n",
        "    segments_path = os.path.join(subset_dir, \"segments\")\n",
        "    text_path = os.path.join(subset_dir, \"text\")\n",
        "\n",
        "    os.makedirs(processed_audio_dir, exist_ok=True)\n",
        "\n",
        "    wav_scp = {}\n",
        "    with open(wav_scp_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            file_name, audio_path = line.strip().split(maxsplit=1)\n",
        "            wav_scp[file_name] = audio_path\n",
        "\n",
        "    segments = []\n",
        "    with open(segments_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            utt_id, file_name, start_time, end_time = line.strip().split()\n",
        "            segments.append({\n",
        "                \"utt_id\": utt_id,\n",
        "                \"file_name\": file_name,\n",
        "                \"start_time\": float(start_time),\n",
        "                \"end_time\": float(end_time),\n",
        "            })\n",
        "\n",
        "    text_data = {}\n",
        "    with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            utt_id, transcription = line.strip().split(maxsplit=1)\n",
        "            text_data[utt_id] = transcription\n",
        "\n",
        "    for seg in tqdm(segments, desc=f\"Processing {subset}\"):\n",
        "        utt_id = seg[\"utt_id\"]\n",
        "        file_name = seg[\"file_name\"]\n",
        "        start_time = seg[\"start_time\"]\n",
        "        end_time = seg[\"end_time\"]\n",
        "        transcription = text_data.get(utt_id, \"\")\n",
        "\n",
        "        if file_name in wav_scp:\n",
        "            audio_path = wav_scp[file_name]\n",
        "            try:\n",
        "                y, sr = librosa.load(audio_path, sr=16000, offset=start_time, duration=(end_time - start_time))\n",
        "                output_audio_path = os.path.join(processed_audio_dir, f\"{utt_id}.wav\")\n",
        "                sf.write(output_audio_path, y, 16000)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {audio_path}: {e}\")\n",
        "\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/preprocessed_audio\"\n",
        "output_dir = \"/content/dirve/MyDrive\"\n",
        "processed_audio_dir = \"/content/drive/MyDrive/processed_audio\"\n",
        "os.makedirs(processed_audio_dir, exist_ok=True)\n",
        "\n",
        "for subset in [\"dev\"]:\n",
        "    subset_audio_dir = os.path.join(processed_audio_dir, subset)\n",
        "    os.makedirs(subset_audio_dir, exist_ok=True)\n",
        "    split_audio_and_generate_metadata(subset, base_dir, output_dir, subset_audio_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBZLm-STnc2b"
      },
      "source": [
        "Following this, we can upload our dataset into ```huggingface_hub```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P4jg49UnTOM"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGOjj-Xtnmg5"
      },
      "source": [
        "After this, just paste a token with *write* access to your hugging face account.\n",
        "\n",
        "First we wuill create a ```DatasetDict``` object for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOXRa5DDnFJz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import Dataset, DatasetDict, Audio\n",
        "\n",
        "audio_dict = {\n",
        "    \"train\": {\"audio\": []},\n",
        "    \"test\": {\"audio\": []},\n",
        "    \"validation\": {\"audio\": []},\n",
        "}\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/processed_audio\"\n",
        "\n",
        "for split, key in zip([\"train\", \"eval\", \"dev\"], [\"train\", \"test\", \"validation\"]):\n",
        "    split_path = os.path.join(base_path, split)\n",
        "\n",
        "    if os.path.exists(split_path):\n",
        "        for file in os.listdir(split_path):\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_path = os.path.join(split_path, file)\n",
        "                audio_dict[key][\"audio\"].append(file_path)\n",
        "\n",
        "train_dataset = Dataset.from_dict(audio_dict[\"train\"]).cast_column(\"audio\", Audio())\n",
        "test_dataset = Dataset.from_dict(audio_dict[\"test\"]).cast_column(\"audio\", Audio())\n",
        "validation_dataset = Dataset.from_dict(audio_dict[\"validation\"]).cast_column(\"audio\", Audio())\n",
        "\n",
        "audio_dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"test\": test_dataset,\n",
        "    \"validation\": validation_dataset\n",
        "})\n",
        "\n",
        "print(audio_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUp8Yiv7n-MK"
      },
      "source": [
        "You should get an output like this\n",
        "\n",
        "```python\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['audio'],\n",
        "        num_rows: 71058\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['audio'],\n",
        "        num_rows: 1994\n",
        "    })\n",
        "    validation: Dataset({\n",
        "        features: ['audio'],\n",
        "        num_rows: 7983\n",
        "    })\n",
        "})\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vd4eRa6oJQy"
      },
      "source": [
        "Now, just push the dataset into hugging face using the ```push_to_hub()``` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjeILm9oob2s"
      },
      "outputs": [],
      "source": [
        "audio_dataset.push_to_hub(\"haideraqeeb/gujrati-asr-16kHz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plo51wH6o3-W"
      },
      "source": [
        "This will be enough for creating a dataset and pushing it into hugging face."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
